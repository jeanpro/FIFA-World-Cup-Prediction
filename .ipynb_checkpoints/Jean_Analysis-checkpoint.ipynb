{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Model - FIFA World Cup Prediction\n",
    "\n",
    "# Projeto\n",
    "__Objetivo__:\n",
    "\n",
    "- Usar um modelo de redes neurais para prever os resultados da Copa do Mundo de 2018\n",
    "\n",
    "Fonte: Linh Truong - https://github.com/mrthlinh/FIFA-World-Cup-Prediction\n",
    "\n",
    "__Dados__: Os dados foram coletados de várias fontes como, por exemplo, Kaggle (a comunidade do Google para Data Scientists), site da FIFA e da EA games.\n",
    "\n",
    "__Feature Engineering__: Para determinar quem tem mais chances de vencer, foram levados em conta 4 fatores:\n",
    "1. Histórico de confrontos diretos.\n",
    "2. Performance de ambos os times nos últimos 10 jogos.\n",
    "3. O nível de aposta no site http://www.oddsportal.com/soccer/world/world-cup-2018/ para cada time (vitória, empate ou derrota).\n",
    "4. O nível do time que foi à Copa do Mundo no video game (FIFA).\n",
    "\n",
    "\n",
    "# Dados\n",
    "### Fontes:\n",
    "O dataset é composto de dados de jogos internacionais, resultados, níveis de aposta, ranking da FIFA, e nível do time no video game FIFA.\n",
    "1. [FIFA World Cup 2018](https://www.kaggle.com/ahmedelnaggar/fifa-worldcup-2018-dataset/data)\n",
    "2. [International match 1872 - 2018](https://www.kaggle.com/martj42/international-football-results-from-1872-to-2017/data)\n",
    "3. [FIFA Ranking through Time](https://www.fifa.com/fifa-world-ranking/ranking-table/men/index.html)\n",
    "4. [Bet Odd](https://www.kaggle.com/austro/beat-the-bookie-worldwide-football-dataset/data)\n",
    "5. [Bet Odd 2](http://www.oddsportal.com)\n",
    "6. [Squad Strength - Sofia](https://sofifa.com/players/top)\n",
    "7. [Squad Strength - FIFA index](https://www.fifaindex.com/)\n",
    "\n",
    "\n",
    "\n",
    "[1]: https://www.kaggle.com/ahmedelnaggar/fifa-worldcup-2018-dataset/data\n",
    "[2]: https://www.kaggle.com/martj42/international-football-results-from-1872-to-2017/data\n",
    "[3]: https://www.fifa.com/fifa-world-ranking/ranking-table/men/index.html\n",
    "[4]: https://www.kaggle.com/austro/beat-the-bookie-worldwide-football-dataset/data\n",
    "[5]: http://www.oddsportal.com\n",
    "[6]: https://sofifa.com/players/top\n",
    "[7]: https://www.fifaindex.com/\n",
    "\n",
    "# Resultados preliminares\n",
    "\n",
    "__Neural Net__: O modelo de redes neurais do autor mostrou-se com precisão de 60.80% no teste 10-fold CV.\n",
    "\n",
    "|           Model         |10-fold CV error rate (%)|\n",
    "|:-----------------------:|:-------------------:|\n",
    "|Neural Net               |60.80|\n",
    "\n",
    "\n",
    "\n",
    "### Resultados da Copa do Mundo 2018\n",
    "Aplicando o modelo para os primeiros jogos da Copa na Rússia chegamos aos resultados abaixo. Enquanto o modelo não se mostrou tão preciso na fase de testes ele mostra resultados surpreendentes nos primeiros jogos da copa. Até o momento desta publicação foram 8 acertos em 14 jogos incluíndo jogos como __Portugal - Espanha (3-3)__, __Brasil - Suíça (1-1)__.\n",
    "\n",
    "__Explicação dos resultados:__\n",
    "\n",
    "Time A vs Time B\n",
    "\n",
    "- \"win_1\": Time A vence com 1 gol de diferença.\n",
    "- \"win_2\": Time A vence com 2 gols de diferença.\n",
    "- \"win_3\": Time A vence com 3 ou mais gols de diferença.\n",
    "- \"lose_1\": Time B vence com 1 gol de diferença.\n",
    "- \"lose_2\": Time B vence com 2 gols de diferença.\n",
    "- \"lose_3\": Time B vence com 3 ou mais gols de diferença.\n",
    "- \"draw_0\": Empate\n",
    "\n",
    "__Match Day 1__\n",
    "![](https://github.com/mrthlinh/FIFA-World-Cup-Prediction/blob/master/pic/WC_2018_matchday1.PNG)\n",
    "\n",
    "Accuracy = 8 / 16\n",
    "\n",
    "__Match Day 2__\n",
    "![](https://github.com/mrthlinh/FIFA-World-Cup-Prediction/blob/master/pic/WC_2018_matchday2.PNG)\n",
    "\n",
    "\n",
    "# Referências\n",
    "1. [A machine learning framework for sport result prediction](https://www.sciencedirect.com/science/article/pii/S2210832717301485)\n",
    "2. [t-test definition](https://en.wikipedia.org/wiki/Student%27s_t-test)\n",
    "3. [Confusion Matrix Multi-Label example](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py)\n",
    "4. [Precision-Recall Multi-Label example](http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#in-multi-label-settings)\n",
    "5. [ROC curve example](http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py)\n",
    "6. [Model evaluation](http://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics)\n",
    "7. [Tuning the hyper-parameters of an estimator](http://scikit-learn.org/stable/modules/grid_search.html)\n",
    "8. [Validation curves](http://scikit-learn.org/stable/modules/learning_curve.html)\n",
    "9. [Understand Bet odd format](https://www.pinnacle.com/en/betting-articles/educational/odds-formats-available-at-pinnacle-sports/ZWSJD9PPX69V3YXZ)\n",
    "10. [EURO 2016 bet odd](http://www.oddsportal.com/soccer/europe/euro-2016/results/#/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load library\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "#from LE import saveLabelEncoder,loadLabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loadData(data, scaler = True, home_team = True,test_size=0.3):\n",
    "    data_ = data.iloc[:,2:]\n",
    "    x = data_.iloc[:,:-1]    \n",
    "    y = data_.iloc[:,-1]\n",
    "    \n",
    "    # Label Encoder 'Result'\n",
    "    encoder = LabelEncoder()    \n",
    "    y = encoder.fit_transform(y)\n",
    "    \n",
    "    if home_team:\n",
    "        same_ht = x.team_1 == x.home_team\n",
    "        x.loc[same_ht,'home_team'] = 1\n",
    "        x.loc[-same_ht,'home_team'] = 0\n",
    "    else:\n",
    "        x = x.drop(['home_team'], axis=1)\n",
    "    x = x.drop(['team_1','team_2','tournament'], axis=1)\n",
    "    \n",
    "    if scaler:\n",
    "        x.iloc[:,1:] = StandardScaler().fit_transform(x.iloc[:,1:])\n",
    "        \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y.squeeze(),test_size=test_size, random_state=85)\n",
    "    \n",
    "    return [x,y,x_train, x_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def saveLabelEncoder(x,file):\n",
    "    encoder = LabelEncoder()    \n",
    "    encoder.fit(x)\n",
    "    np.save(file, encoder.classes_)\n",
    "    return encoder\n",
    "\n",
    "def loadLabelEncoder(file):\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.classes_ = np.load(file)\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data processing\n",
    "\"\"\"\n",
    "Created on Sat May 26 23:45:38 2018\n",
    "\n",
    "@author: mrthl\n",
    "\"\"\"\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"data/data_odd_2005.csv\", encoding='utf-8')\n",
    "data_x, data_y, x_train, x_test, y_train, y_test = loadData(data)\n",
    "list_data = [data_x, data_y, x_train, x_test, y_train, y_test]\n",
    "\n",
    "data_full = pd.read_csv(\"data/international-football-results-from-1872-to-2017.csv\")\n",
    "data_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Neural Net\n",
    "\n",
    "# Fit GradientBoosting Tree with GridSearch CV:\n",
    "layer = (1000,500)\n",
    "model_NN = MLPClassifier(hidden_layer_sizes = layer, max_iter = 1000, alpha=1e-4,\n",
    "                    solver='adam', verbose=10, tol=1e-10, random_state=1,            \n",
    "                    learning_rate_init=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Result reportting\n",
    "\"\"\"\n",
    "Created on Tue May  8 12:12:59 2018\n",
    "\n",
    "@author: mrthl\n",
    "\"\"\"\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy import interp\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "def MyReport(model,model_Name,list_data,tune = True):  \n",
    "    data_x, data_y, x_train, x_test, y_train, y_test = list_data\n",
    "    # Training\n",
    "    model.fit(x_train,y_train)\n",
    "    \n",
    "    modelCV = model\n",
    "    if tune:                             \n",
    "        modelCV = model.best_estimator_\n",
    "\n",
    "    # General Report          \n",
    "    y_predCV = modelCV.predict(x_test)\n",
    "    print(classification_report(y_test, y_predCV))\n",
    "     \n",
    "    # Plot Confusion Matrix\n",
    "    le_result = loadLabelEncoder('LE/result.npy')\n",
    "    class_names = le_result.classes_\n",
    "    cnf_matrix = confusion_matrix(y_test, y_predCV)\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names,title=model_Name+' Confusion matrix, without normalization')\n",
    "    \n",
    "    # ROC curve\n",
    "    try:\n",
    "        y_score = modelCV.decision_function(x_test)\n",
    "        plot_ROC_curve(y_test,y_score,title=model_Name+' ROC curve',class_names = class_names)\n",
    "    except:\n",
    "        print(\"ROC curve is not available because model does not have decision_function method\")\n",
    "    # 10-fold-test error\n",
    "    scores = cross_val_score(modelCV, data_x, data_y, cv=10)\n",
    "    print(\"10-fold cross validation mean square error: \",np.mean(scores))\n",
    "    return modelCV\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def plot_ROC_curve(y_test,y_score,title,class_names):\n",
    "    \n",
    "    # Binarize the output\n",
    "#    y = label_binarize(y_test, classes=[0, 1, 2])\n",
    "    y = label_binarize(y_test, classes=[0, 1, 2])\n",
    "    n_classes = y.shape[1]\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    lw = 2\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    \n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "    \n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "    \n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    # Plot all ROC curves\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "    \n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "    \n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "\n",
    "#    for i, color in zip(class_names, colors):\n",
    "#        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "#                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "#                 ''.format(i, roc_auc[i]))\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of class \"{0}\" (area = {1:0.2f})'\n",
    "                 ''.format(class_names[i], roc_auc[i]))\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.20229321\n",
      "Iteration 2, loss = 1.08681901\n",
      "Iteration 3, loss = 1.02984474\n",
      "Iteration 4, loss = 1.07281382\n",
      "Iteration 5, loss = 1.00104737\n",
      "Iteration 6, loss = 0.98956284\n",
      "Iteration 7, loss = 1.01330235\n",
      "Iteration 8, loss = 1.00806867\n",
      "Iteration 9, loss = 1.14902161\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       418\n",
      "          1       0.44      0.97      0.60       677\n",
      "          2       0.72      0.24      0.36       627\n",
      "\n",
      "avg / total       0.43      0.47      0.37      1722\n",
      "\n",
      "Confusion matrix, without normalization\n",
      "[[  0 377  41]\n",
      " [  0 658  19]\n",
      " [  0 474 153]]\n",
      "ROC curve is not available because model does not have decision_function method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jean/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.94182344\n",
      "Iteration 2, loss = 1.04038879\n",
      "Iteration 3, loss = 1.00319608\n",
      "Iteration 4, loss = 0.99197275\n",
      "Iteration 5, loss = 1.01643766\n",
      "Iteration 6, loss = 0.98861149\n",
      "Iteration 7, loss = 0.96653836\n",
      "Iteration 8, loss = 0.94273071\n",
      "Iteration 9, loss = 0.97607736\n",
      "Iteration 10, loss = 1.01210754\n",
      "Iteration 11, loss = 1.01774332\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.07432689\n",
      "Iteration 2, loss = 1.05610539\n",
      "Iteration 3, loss = 0.99683271\n",
      "Iteration 4, loss = 0.99533328\n",
      "Iteration 5, loss = 0.97771223\n",
      "Iteration 6, loss = 0.96460499\n",
      "Iteration 7, loss = 0.95877586\n",
      "Iteration 8, loss = 0.92476326\n",
      "Iteration 9, loss = 0.92582622\n",
      "Iteration 10, loss = 0.91276616\n",
      "Iteration 11, loss = 0.90116385\n",
      "Iteration 12, loss = 0.89518638\n",
      "Iteration 13, loss = 0.89652908\n",
      "Iteration 14, loss = 0.89746326\n",
      "Iteration 15, loss = 0.91293187\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.19559755\n",
      "Iteration 2, loss = 0.99783176\n",
      "Iteration 3, loss = 1.00412858\n",
      "Iteration 4, loss = 0.98261324\n",
      "Iteration 5, loss = 0.96547057\n",
      "Iteration 6, loss = 0.94704339\n",
      "Iteration 7, loss = 0.93344947\n",
      "Iteration 8, loss = 0.92303030\n",
      "Iteration 9, loss = 0.90840606\n",
      "Iteration 10, loss = 0.91720160\n",
      "Iteration 11, loss = 0.91524524\n",
      "Iteration 12, loss = 0.92665534\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.33388394\n",
      "Iteration 2, loss = 1.06371310\n",
      "Iteration 3, loss = 1.01754797\n",
      "Iteration 4, loss = 0.97867977\n",
      "Iteration 5, loss = 0.98275295\n",
      "Iteration 6, loss = 1.00561547\n",
      "Iteration 7, loss = 1.01426851\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.13158167\n",
      "Iteration 2, loss = 1.05796492\n",
      "Iteration 3, loss = 1.01708048\n",
      "Iteration 4, loss = 0.98191717\n",
      "Iteration 5, loss = 0.97544322\n",
      "Iteration 6, loss = 0.98654190\n",
      "Iteration 7, loss = 0.98309279\n",
      "Iteration 8, loss = 0.94392024\n",
      "Iteration 9, loss = 0.93441656\n",
      "Iteration 10, loss = 0.91724237\n",
      "Iteration 11, loss = 0.90615626\n",
      "Iteration 12, loss = 0.90403464\n",
      "Iteration 13, loss = 0.90521448\n",
      "Iteration 14, loss = 0.92528232\n",
      "Iteration 15, loss = 0.90812804\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.23783253\n",
      "Iteration 2, loss = 1.01901326\n",
      "Iteration 3, loss = 0.99548703\n",
      "Iteration 4, loss = 0.96716472\n",
      "Iteration 5, loss = 0.97918456\n",
      "Iteration 6, loss = 0.97685196\n",
      "Iteration 7, loss = 0.98776546\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.61208773\n",
      "Iteration 2, loss = 1.07776731\n",
      "Iteration 3, loss = 1.02345729\n",
      "Iteration 4, loss = 0.99846723\n",
      "Iteration 5, loss = 1.03179529\n",
      "Iteration 6, loss = 1.04122258\n",
      "Iteration 7, loss = 0.99912740\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.71747239\n",
      "Iteration 2, loss = 1.05677919\n",
      "Iteration 3, loss = 1.02965929\n",
      "Iteration 4, loss = 0.98764901\n",
      "Iteration 5, loss = 0.97427222\n",
      "Iteration 6, loss = 0.98555781\n",
      "Iteration 7, loss = 0.94776825\n",
      "Iteration 8, loss = 0.94392182\n",
      "Iteration 9, loss = 0.92729595\n",
      "Iteration 10, loss = 0.91796646\n",
      "Iteration 11, loss = 0.91986420\n",
      "Iteration 12, loss = 0.91358809\n",
      "Iteration 13, loss = 0.91621406\n",
      "Iteration 14, loss = 0.90647200\n",
      "Iteration 15, loss = 1.00003270\n",
      "Iteration 16, loss = 1.00684061\n",
      "Iteration 17, loss = 0.95835744\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.51773965\n",
      "Iteration 2, loss = 1.06095791\n",
      "Iteration 3, loss = 1.01801024\n",
      "Iteration 4, loss = 0.97555494\n",
      "Iteration 5, loss = 0.99039050\n",
      "Iteration 6, loss = 0.98857010\n",
      "Iteration 7, loss = 0.96415336\n",
      "Iteration 8, loss = 0.95136566\n",
      "Iteration 9, loss = 0.96042493\n",
      "Iteration 10, loss = 0.94210167\n",
      "Iteration 11, loss = 0.91884773\n",
      "Iteration 12, loss = 0.91813926\n",
      "Iteration 13, loss = 0.90605166\n",
      "Iteration 14, loss = 0.91523084\n",
      "Iteration 15, loss = 0.89365813\n",
      "Iteration 16, loss = 0.90436151\n",
      "Iteration 17, loss = 0.93068281\n",
      "Iteration 18, loss = 0.94001226\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.37195839\n",
      "Iteration 2, loss = 1.10194833\n",
      "Iteration 3, loss = 1.09682238\n",
      "Iteration 4, loss = 1.10553341\n",
      "Iteration 5, loss = 1.08054536\n",
      "Iteration 6, loss = 1.11037411\n",
      "Iteration 7, loss = 1.12034493\n",
      "Iteration 8, loss = 1.10975009\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "('10-fold cross validation mean square error: ', 0.51488413531994337)\n"
     ]
    }
   ],
   "source": [
    "#Display results\n",
    "\n",
    "\n",
    "# Fit cross-validation and report result\n",
    "modelCV_NN = MyReport(model = model_NN, model_Name = 'Neural Network', list_data = list_data,tune = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Neural Net\n",
    "\n",
    "\n",
    "This section allows us to use the model based on the following inputs:\n",
    "\n",
    "* Team1 and Team2\n",
    "* Avg Bet for the teams (win1, draw, win2) based on (http://www.oddsportal.com/soccer/world/world-cup-2018/)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def processInput (input_array):\n",
    "    #inputs\n",
    "    team_1 = input_array[0]\n",
    "    team_2 = input_array[1]\n",
    "    avg_odds_win_1 = input_array[2] \n",
    "    avg_odds_draw = input_array[3] \n",
    "    avg_odds_win_2 = input_array[4]\n",
    "    print(team_1)\n",
    "    if any(a is None for a in [team_1,team_2, avg_odds_win_1,avg_odds_draw, avg_odds_win_2]):\n",
    "        print (\"Some inputs are empty. Please enter valid inputs.\")\n",
    "        return\n",
    "\n",
    "    df1 = data_full[(data_full.home_team == team_1) & (data_full.away_team == team_2)]\n",
    "    df2 = data_full[(data_full.home_team == team_2) & (data_full.away_team == team_1)]\n",
    "    df3 = df1.append(df2).sort_values(by=['date'])\n",
    "    df3['winner'] = np.where(df3.home_score > df3.away_score, df3.home_team, \n",
    "                                     np.where(df3.home_score < df3.away_score, df3.away_team, 'None'))\n",
    "    df3.head()\n",
    "\n",
    "    # define our own dataframe\n",
    "    col_names = ['home_team', 'h_win_diff', 'h_draw', 'f_goalF_1', \n",
    "                 'f_goalF_2','f_goalA_1','f_goalA_2', 'f_win_1', 'f_win_2', 'f_draw_1', 'f_draw_2','avg_odds_win_1', 'avg_odds_draw', 'avg_odds_win_2']\n",
    "    my_df = pd.DataFrame(columns = col_names)\n",
    "\n",
    "    # Test history match stat    \n",
    "    if len(df3) != 0:\n",
    "        curr_match = df3.iloc[-1,:]\n",
    "        df_match = df3[df3['date'] < curr_match['date'] ]\n",
    "        count = Counter(df_match['winner']) #Now we get no.win and no.draw\n",
    "\n",
    "        pre_df = curr_match[['id','date','tournament']].copy()\n",
    "\n",
    "        #     \tid\tdate\tteam_1\tteam_2\thome_team\ttournament\th_win_diff\th_draw\tf_goalF_1\tf_goalF_2\tf_goalA_1\tf_goalA_2\tf_win_1\tf_win_2\tf_draw_1\tf_draw_2\tresult\n",
    "        # Get the head-to-head result\n",
    "        pre_df['team_1'] =  curr_match['home_team'] if curr_match['home_team'] < curr_match['away_team'] else curr_match['away_team'] \n",
    "        pre_df['team_2'] =  curr_match['away_team'] if curr_match['home_team'] < curr_match['away_team'] else curr_match['home_team'] \n",
    "        pre_df['home_team'] = curr_match['home_team']\n",
    "\n",
    "        pre_df['h_win_diff'] = count[pre_df['team_1']] - count[pre_df['team_2']]\n",
    "        pre_df['h_draw'] = count['None']\n",
    "    else:\n",
    "        initial_data = {'home_team':team_1,'team_1':team_1,'team_2':team_2,'h_win_diff':0,'h_draw':0}\n",
    "        pre_df = pd.Series(initial_data)\n",
    "        \n",
    "    # Get the recent result of each team\n",
    "    # Team 1\n",
    "    df_10  = data_full[(data_full.home_team == team_1) | (data_full.away_team == team_1)]\n",
    "\n",
    "    length = df_10.shape[0]\n",
    "    df_10  = df_10.iloc[length-11:length-1,:]\n",
    "\n",
    "    df_10['winner'] = np.where(df_10.home_score > df_10.away_score, df_10.home_team, \n",
    "                                     np.where(df_10.home_score < df_10.away_score, df_10.away_team, 'None'))\n",
    "\n",
    "    pre_df['f_goalF_1'] = df_10['home_score'].sum()\n",
    "    pre_df['f_goalA_1'] = df_10['away_score'].sum()\n",
    "\n",
    "    dict_result_10 = Counter(df_10['winner'])\n",
    "    pre_df['f_win_1']  = dict_result_10[team_1]\n",
    "    pre_df['f_draw_1'] = dict_result_10['None']\n",
    "\n",
    "    # Team 2\n",
    "    df_10  = data_full[(data_full.home_team == team_2) | (data_full.away_team == team_2)]\n",
    "\n",
    "    length = df_10.shape[0]\n",
    "    df_10  = df_10.iloc[length-11:length-1,:]\n",
    "\n",
    "    df_10['winner'] = np.where(df_10.home_score > df_10.away_score, df_10.home_team, \n",
    "                                     np.where(df_10.home_score < df_10.away_score, df_10.away_team, 'None'))\n",
    "\n",
    "    pre_df['f_goalF_2'] = df_10['home_score'].sum()\n",
    "    pre_df['f_goalA_2'] = df_10['away_score'].sum()\n",
    "\n",
    "    dict_result_10 = Counter(df_10['winner'])\n",
    "    pre_df['f_win_2']  = dict_result_10[team_2]\n",
    "    pre_df['f_draw_2'] = dict_result_10['None']\n",
    "    pre_df['avg_odds_win_1'] = avg_odds_win_1\n",
    "    pre_df['avg_odds_draw'] = avg_odds_draw\n",
    "    pre_df['avg_odds_win_2'] = avg_odds_win_2\n",
    "    \n",
    "    #encode\n",
    "    #le_tournament = loadLabelEncoder('LE/tournament.npy')\n",
    "    #pre_df['tournament'] = le_tournament.transform([pre_df['tournament']])[0]\n",
    "\n",
    "    # Add HOME team\n",
    "    same_ht = pre_df.team_1 == pre_df.home_team\n",
    "    if same_ht:\n",
    "        pre_df.loc['home_team'] = 1\n",
    "    else:\n",
    "        pre_df.loc['home_team'] = 0\n",
    "    \n",
    "    my_df = my_df.append(pre_df[col_names],ignore_index=True)\n",
    "\n",
    "    return my_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs para os próximos jogos\n",
    "\n",
    "Nesta seção, vamos entrar com os dados para os próximos jogos e verificar os resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poland\n",
      "  home_team h_win_diff h_draw f_goalF_1 f_goalF_2 f_goalA_1 f_goalA_2 f_win_1  \\\n",
      "0         0          0      0        21         7        14        16       5   \n",
      "\n",
      "  f_win_2 f_draw_1 f_draw_2  avg_odds_win_1  avg_odds_draw  avg_odds_win_2  \n",
      "0       3        2        5            3.49           3.47            2.21  \n",
      "Saudi Arabia\n",
      "  home_team h_win_diff h_draw f_goalF_1 f_goalF_2 f_goalA_1 f_goalA_2 f_win_1  \\\n",
      "0         0          1      2        23        12         9         5       3   \n",
      "\n",
      "  f_win_2 f_draw_1 f_draw_2  avg_odds_win_1  avg_odds_draw  avg_odds_win_2  \n",
      "0       2        1        3            4.61           3.53            1.89  \n",
      "Uruguay\n",
      "  home_team h_win_diff h_draw f_goalF_1 f_goalF_2 f_goalA_1 f_goalA_2 f_win_1  \\\n",
      "0         1          5      0        12        17         7        16       6   \n",
      "\n",
      "  f_win_2 f_draw_1 f_draw_2  avg_odds_win_1  avg_odds_draw  avg_odds_win_2  \n",
      "0       2        3        3            2.62           3.07            3.09  \n",
      "Iran\n",
      "  home_team h_win_diff h_draw f_goalF_1 f_goalF_2 f_goalA_1 f_goalA_2 f_win_1  \\\n",
      "0         0         -1      0        13        16         7        12       7   \n",
      "\n",
      "  f_win_2 f_draw_1 f_draw_2  avg_odds_win_1  avg_odds_draw  avg_odds_win_2  \n",
      "0       5        1        4            6.08           3.68            1.68  \n",
      "Spain\n",
      "  home_team h_win_diff h_draw f_goalF_1 f_goalF_2 f_goalA_1 f_goalA_2 f_win_1  \\\n",
      "0         0         -1      0        22         9        19        13       6   \n",
      "\n",
      "  f_win_2 f_draw_1 f_draw_2  avg_odds_win_1  avg_odds_draw  avg_odds_win_2  \n",
      "0       7        4        2            1.35           5.03           10.59  \n",
      "Australia\n",
      "  home_team h_win_diff h_draw f_goalF_1 f_goalF_2 f_goalA_1 f_goalA_2 f_win_1  \\\n",
      "0         1          0      0        15        10        11         7       5   \n",
      "\n",
      "  f_win_2 f_draw_1 f_draw_2  avg_odds_win_1  avg_odds_draw  avg_odds_win_2  \n",
      "0       6        3        4            3.05           3.35            2.47  \n",
      "Denmark\n",
      "  home_team h_win_diff h_draw f_goalF_1 f_goalF_2 f_goalA_1 f_goalA_2 f_win_1  \\\n",
      "0         1         -3      1         6        15        12        12       5   \n",
      "\n",
      "  f_win_2 f_draw_1 f_draw_2  avg_odds_win_1  avg_odds_draw  avg_odds_win_2  \n",
      "0       6        4        3            4.55           2.64             2.3  \n",
      "Iceland\n",
      "  home_team h_win_diff h_draw f_goalF_1 f_goalF_2 f_goalA_1 f_goalA_2 f_win_1  \\\n",
      "0         0          4      1        15        11        22         8       4   \n",
      "\n",
      "  f_win_2 f_draw_1 f_draw_2  avg_odds_win_1  avg_odds_draw  avg_odds_win_2  \n",
      "0       5        2        2             3.5           3.39             2.2  \n",
      "Nigeria\n",
      "  home_team h_win_diff h_draw f_goalF_1 f_goalF_2 f_goalA_1 f_goalA_2 f_win_1  \\\n",
      "0         1          4      1        17        14         8        18       4   \n",
      "\n",
      "  f_win_2 f_draw_1 f_draw_2  avg_odds_win_1  avg_odds_draw  avg_odds_win_2  \n",
      "0       5        2        3            6.61           4.38            1.54  \n",
      "Mexico\n",
      "  home_team h_win_diff h_draw f_goalF_1 f_goalF_2 f_goalA_1 f_goalA_2 f_win_1  \\\n",
      "0         1         -1      3        16        14         8         4       5   \n",
      "\n",
      "  f_win_2 f_draw_1 f_draw_2  avg_odds_win_1  avg_odds_draw  avg_odds_win_2  \n",
      "0       3        2        4            2.42            3.3            3.17  \n",
      "South Korea\n",
      "  home_team h_win_diff h_draw f_goalF_1 f_goalF_2 f_goalA_1 f_goalA_2 f_win_1  \\\n",
      "0         1          0      0         0        20         0        12       0   \n",
      "\n",
      "  f_win_2 f_draw_1 f_draw_2  avg_odds_win_1  avg_odds_draw  avg_odds_win_2  \n",
      "0       5        0        3           15.76            7.6            1.19  \n",
      "Serbia\n",
      "  home_team h_win_diff h_draw f_goalF_1 f_goalF_2 f_goalA_1 f_goalA_2 f_win_1  \\\n",
      "0         1          2      1        16         7        10        13       6   \n",
      "\n",
      "  f_win_2 f_draw_1 f_draw_2  avg_odds_win_1  avg_odds_draw  avg_odds_win_2  \n",
      "0       7        1        3            7.76           4.39             1.5  \n",
      "Switzerland\n",
      "  home_team h_win_diff h_draw f_goalF_1 f_goalF_2 f_goalA_1 f_goalA_2 f_win_1  \\\n",
      "0         0         -1      0        19        20         8         5       7   \n",
      "\n",
      "  f_win_2 f_draw_1 f_draw_2  avg_odds_win_1  avg_odds_draw  avg_odds_win_2  \n",
      "0       2        2        2            1.78           3.29            5.91  \n",
      "Japan\n",
      "  home_team h_win_diff h_draw f_goalF_1 f_goalF_2 f_goalA_1 f_goalA_2 f_win_1  \\\n",
      "0         0         -3      0        15        21        14        14       3   \n",
      "\n",
      "  f_win_2 f_draw_1 f_draw_2  avg_odds_win_1  avg_odds_draw  avg_odds_win_2  \n",
      "0       5        1        2            2.71           3.28            2.71  \n",
      "Senegal\n",
      "  home_team h_win_diff h_draw f_goalF_1 f_goalF_2 f_goalA_1 f_goalA_2 f_win_1  \\\n",
      "0         1          1      0         7         7        11        16       4   \n",
      "\n",
      "  f_win_2 f_draw_1 f_draw_2  avg_odds_win_1  avg_odds_draw  avg_odds_win_2  \n",
      "0       3        5        5            3.86           3.38            2.05  \n",
      "England\n",
      "  home_team h_win_diff h_draw f_goalF_1 f_goalF_2 f_goalA_1 f_goalA_2 f_win_1  \\\n",
      "0         0        -14      5         8        32         9        10       7   \n",
      "\n",
      "  f_win_2 f_draw_1 f_draw_2  avg_odds_win_1  avg_odds_draw  avg_odds_win_2  \n",
      "0       8        3        2            2.88           3.14            2.67  \n",
      "Panama\n",
      "  home_team h_win_diff h_draw f_goalF_1 f_goalF_2 f_goalA_1 f_goalA_2 f_win_1  \\\n",
      "0         1          0      0        17        12         9        12       3   \n",
      "\n",
      "  f_win_2 f_draw_1 f_draw_2  avg_odds_win_1  avg_odds_draw  avg_odds_win_2  \n",
      "0       4        2        4            4.52           3.66            1.83  \n",
      "[[u'lose']\n",
      " [u'lose']\n",
      " [u'lose']\n",
      " [u'lose']\n",
      " [u'lose']\n",
      " [u'lose']\n",
      " [u'lose']\n",
      " [u'lose']\n",
      " [u'lose']\n",
      " [u'lose']\n",
      " [u'lose']\n",
      " [u'lose']\n",
      " [u'lose']\n",
      " [u'lose']\n",
      " [u'lose']\n",
      " [u'lose']\n",
      " [u'lose']]\n"
     ]
    }
   ],
   "source": [
    "#Inputs\n",
    "\n",
    "le_result = loadLabelEncoder('LE/result.npy')\n",
    "\n",
    "inputs = [[\"Poland\",\"Colombia\", 3.49, 3.47, 2.21],\n",
    "          [\"Saudi Arabia\",\"Egypt\", 4.61,3.53,1.89],\n",
    "          [\"Uruguay\",\"Russia\", 2.62,3.07,3.09],\n",
    "          [\"Iran\",\"Portugal\",6.08,3.68,1.68],\n",
    "          [\"Spain\",\"Morocco\",1.35,5.03,10.59],\n",
    "          [\"Australia\",\"Peru\",3.05,3.35,2.47],\n",
    "          [\"Denmark\",\"France\",4.55,2.64,2.30],\n",
    "          [\"Iceland\",\"Croatia\",3.50,3.39,2.20],\n",
    "          [\"Nigeria\",\"Argentina\",6.61,4.38,1.54],\n",
    "          [\"Mexico\",\"Sweden\",2.42,3.30,3.17],\n",
    "          [\"South Korea\",\"Germany\",15.76,7.60,1.19],\n",
    "          [\"Serbia\",\"Brazil\",7.76,4.39,1.50],\n",
    "          [\"Switzerland\",\"Costa Rica\",1.78,3.29,5.91],\n",
    "          [\"Japan\",\"Poland\",2.71,3.28,2.71],\n",
    "          [\"Senegal\",\"Colombia\",3.86,3.38,2.05],\n",
    "          [\"England\",\"Belgium\",2.88,3.14,2.67],\n",
    "          [\"Panama\",\"Tunisia\",4.52,3.66,1.83]          \n",
    "         ]\n",
    "results = []\n",
    "\n",
    "for i in inputs:\n",
    "    \n",
    "    data_fit = processInput(i)\n",
    "    result = modelCV_NN.predict(data_fit)\n",
    "    results.append(result)\n",
    "\n",
    "print(le_result.inverse_transform(results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1])]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "filename = 'save_model/NN.sav'\n",
    "pickle.dump(modelCV_NN, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
